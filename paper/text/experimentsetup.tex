%!TEX root = ../authorinstr.tex

\section{Experimental setup}

In this section, the experimental setup used to test the different algorithms will be described. GD-SARSA, CACLA and NFAC are compared in two continuous environments: MountainCar\cite{openaimountaincar} and LunarLander\cite{openailunarlander}. Both environments are OpenAI Gym environments\cite{openaigym}, which is a toolkit for comparing reinforcement learning algorithms.  Agent performance is measured by looking at the average reward over the best 100 epochs. The reward function is given by the OpenAI environment and shown in more detail in their respective subsections. Each simulation run consisted of 2000 epochs, each of which had a maximum of 10000 time steps. Learning rates, discount factors and numbers of hidden nodes for the MountainCar and LunarLander environments can be found in table \ref{tab:mntparam} and table \ref{tab:lunarparam} respectively. 

\begin{table}
\centering
\label{tab:mntparam}
\begin{tabular}{r|llll}
                     & learning rate & discount factor & number of hidden nodes \\\hline
SARSA & N          & P               & M         \\
CACLA & A          & B               & actor: F, critic: G         \\
NFAC    & X          & Y              & actor: H, critic: I        
\end{tabular}
\caption{MountainCar network parameters per algorithm}
\end{table}

\begin{table}
\centering
\label{tab:lunarparam}
\begin{tabular}{r|llll}
                     & learning rate & discount factor & number of hidden nodes \\\hline
SARSA & N          & P               & M         \\
CACLA & A          & B               & actor: F, critic: G         \\
NFAC    & X          & Y              & actor: H, critic: I        
\end{tabular}
\caption{LunarLander network parameters per algorithm}
\end{table}



GD-SARSA used $\epsilon$-greedy exploration, which was first set to 1.0, resulting initially in a fully pseudo-random behaviour. This exploration rate decayed over epochs, and is equal to $0.99^{N-1}$, where N is the current epoch. Since the experiments run for 2000 epochs, this means that the final exploration rate is $1.86*10^{-9}$.   

NFAC and CACLA both used gaussian exploration.  A random value sampled from a gaussian distribution($\mu=0$ $\sigma=1$) was multiplied by a value $\Sigma=10$. This value was added to the output of the MLP and finally clamped to be in the range [$-1$,$1$]. This $\Sigma$ decayed over epochs and is equal to  $10 * 0.99^{N-1}$, where N is the current epoch. Since the experiments run for 2000 epochs, this means that the final exploration rate is $1.86*10^{-8}$. Since initially the exploration rate is very high, this ensures that the agent will quickly reach its goal. [[The exploration rate is diminshed over time since then it can rely more on its learned behavior rather than the noise added by the gaussion value.]] 

\subsection{MountainCar}
In the MountainCar environment a car is situated in between two mountains. Its goal is to reach the top of the rightmost mountain. In order to do this, it first has to drive up the left mountain in order to generate enough momentum to reach the top of the rightmost mountain. 

The action space of the MountainCar environment contains continouos numbers in the range [$-1$,$1$]. This dictates the force applied to the car in either the left or right direction. The state space consists of two continouos numbers:
\begin{itemize}
    \item[] The car position, in the range [$-1.2$,$0.6$]
    \item[] The car velocity, in the range [$-0.07$,$0.07$]
\end{itemize}

The reward function, where $r_t$ is the reward at time step $t$ and $v_t$ is the car velocity at time step $t$, is given by the OpenAI Gym environment and defined as follows:
\begin{equation}
    r_t =
    \begin{cases*}
      +100 -v_t^2 * 0.1 & if goal is reached \\
      -v_t^2 * 0.1 & otherwise
    \end{cases*}
\end{equation}

The total reward is defined as the sum of all rewards gained during an epoch.

\subsection{LunarLander}
In the LunarLander environment, the lunar lander's goal is to safely land on the lunar surface. It can do this by firing its left or right engine and controlling its overall thrust power. The action space of the LunarLander environment consists of vectors with two continouos numbers in the range [$-1$,$1$]: one for controlling the left or right engine and one for controlling the main engine. In the first case, a value in between -1.0 and -0.5 means the left engine is firing, a value in between 0.5 and 1.0 means the right engine is firing, and any other value means neither engine is firing. For the main engine, a value in between -1.0 and 0 means that the main engine is not firing, while from 0 to 1.0 the engine is throttled from 50\% to 100\% power. The LunarLander state space consists of 6 continouos numbers, x-position, y-position, x-velocity, y-velocity, angle and rotation respectively, in the range [$-1$,$1$] and two booleans variables, indicating ground contact for the left and right leg respectively.

For the reward function, as given by the OpenAI Gym environment, the state at time t $st_t$ is determined. It is defined as such:

\begin{equation} 
    st_t = -100 * d_t - 100 * v_t - 100 * a_t + 10*l_t + 10*r_t
\end{equation}

where $d_t$ is the distance from the lunar lander to landing zone at time $t$, $v_t$ is the velocity of the lunar lander at time $t$, $a_t$ is the lunar lander's angle at time $t$, and $l_t$ and $r_t$ are booleans of respectively the left and right foot touching the ground at time $t$.  

Using this state $st_t$, the change in state which is needed for the reward function can be determined (equation \ref{eq:statet}). 

\begin{equation}
\label{eq:statet}
    \delta _{state} =
    \begin{cases*}
      0  & for epoch 1 \\
      st_t - st _{t-1} & otherwise 
    \end{cases*}
\end{equation}

The reward function, where $r_t$ is the reward at time step $t$, $m_t$ is the main engine power and $lr_t$ is the left-right engine power, is given here (equation \ref{eq:lunarreward}):

\begin{equation}
\label{eq:lunarreward}
    r_t =
    \begin{cases*}
      \delta _{state} - m_t*0.30 - lr_t*0.03 -100  & after crash \\
      \delta _{state} - m_t*0.30 - lr_t*0.03 +100 & after landing \\
      \delta _{state} - m_t*0.30 - lr_t*0.03  & otherwise 
    \end{cases*}
\end{equation}

The total reward is defined as the sum of all rewards gained during an epoch.
