 
\section{Experimental setup}
TODO:::: CONSTRUCT::: (What do we measure) : How well the agent performs
TODO:::: OPERATIONALISATION!!!! (How do we measure it?)!! : Average reward last 100 epochs

We will briefly describe the experimental setup used to test the different algorithms. SARSA, CACLA and NFAC are compared in two continuous environments: MountainCar\cite{openaimountaincar} and LunarLander\cite{openailunarlander}. Both environments are OpenAI Gym environments\cite{openaigym}, which is a toolkit for comparing reinforcement learning algorithms. 

The discount factors used during the experiments were $0.20$, $0.40$, $0.60$, $0.80$, $0.90$, $0.99$ and $0.999$. The learning rates used were $0.001$, $0.01$ and $0.05$.

The number of hidden nodes was varied from $20$ to $200$. Each environment-algorithm pair had its own optimal number of hidden nodes. A table can be found below. 

Each simulation run consists of 2000 epochs, each of which has a maximum of 10000 time steps. 

%%Table?

SARSA-GD used $\epsilon$-greedy exploration, which was first set to 0.1, meaning a $10\%$ chance to take a random action during a time step. This exploration rate decayed over epochs, and is equal to $0.1 * 0.99^{N-1}$, where N is the current epoch. Since our experiments run for 2000 epochs, this means that the final exploration rate is $1.86*10^{-10}$.   

NFAC and CACLA both used gaussian exploration. 

TODODODODO:: In NFAC wordt bij gaussian exploration de sigma alleen maar geupdatet wanneer het goal wordt gehaald in een epoch. In CACLA wordt elke epoch de sigma kleiner gemaakt onafhankelijk van of het goal wordt gehaald. Elk epoch updaten is beter te verdedigen. Daarnaast wordt zoals hierboven te zien is de exploration na like 400 epochs al nagenoeg nul. Als op dat punt nog geen successen zijn behaald convergeert het waarschijnlijk naar bagger. 

%random_chance_vals=(0.1)
%discount_vals=(0.999 0.99 0.90 0.80 0.60 0.40 0.20)
%learning_rate_vals=(0.001 0.01 0.05)
%sigma_vals=(10)
%sd_vals=(1)
%action_hidden_layers=(20 50 100 200)

\subsection{MountainCar}
TODO:: Add image (or in intro)
In the MountainCar environment a car is situated in between two mountains. Its goal is to reach the top of the rightmost mountain. In order to do this, it first has to drive up the left mountain in order to generate enough momentum to reach the top of the rightmost mountain. 

The action space of the MountainCar environment is a float in the range [$-1$,$1$]. This float dictates the force applied to the car in either the left or right direction. The state space consists of two floats:
\begin{itemize}
    \item[] The car position, in the range [$-1.2$,$0.6$]
    \item[] The car velocity, in the range [$-0.07$,$0.07$]
\end{itemize}

The reward function, where $r_t$ is the reward at time step $t$ and $v_t$ is the car velocity at time step $t$, is given by the OpenAI Gym environment and defined as follows:
\begin{equation}
    r_t =
    \begin{cases*}
      +100 -v_t^2 * 0.1 & if goal is reached \\
      -v_t^2 * 0.1 & otherwise
    \end{cases*}
\end{equation}

The total reward is defined as the sum of all rewards gained during an epoch. The MountainCar action space vector is in the $[-1,1]$ range. 

\subsection{LunarLander}
TODO:: Add image (or in intro)
In the LunarLander environment, the lunar lander's goal is to safely land on the lunar surface. It can do this by firing its left or right engine and controlling its overall thrust power. The action space of the LunarLander environment consists of two floats in the range [$-1$,$1$]: one for controlling the left or right engine and one for controlling the main engine. In the first case, a value in between -1.0 and -0.5 means the left engine is firing, a value in between 0.5 and 1.0 means the right engine is firing, and any other value means neither engine is firing. For the main engine, a value in between -1.0 and 0 means that the main engine is not firing, while from 0 to 1.0 the engine is throttled from 50\% to 100\% power. The LunarLander state space consists of 6 floats, all in the range [$-1$,$1$] and two booleans: x\_position, y\_position, x\_velocity, y\_velocity, angle, rotation and the booleans left\_leg\_contact and right\_leg\_contact.  

For the reward function, as given by the OpenAI Gym environment, we read the state at time t $state_t$. It is defined as such:

\begin{equation} 
    state_t = -100 * d_t - 100 * v_t - 100 * a_t + 10*l_t + 10*r_t
\end{equation}

where $d_t$ is the distance from the lunar lander to landing zone at time $t$, $v_t$ is the velocity of the lunar lander at time $t$, $a_t$ is the lunar lander's angle at time $t$, and $l_t$ and $r_t$ are booleans of respectively the left and right foot touching the ground at time $t$.  

Using this $state_t$, we can determine the change in state which is needed for the reward function (equation \ref{eq:lunarreward}). 

\begin{equation}
    \delta _{state} =
    \begin{cases*}
      0  & for epoch 1 \\
      state _t - state _{t-1} & otherwise 
    \end{cases*}
\end{equation}

The reward function, where $r_t$ is the reward at time step $t$, $m_t$ is the main engine power and $lr_t$ is the left-right engine power.

\begin{equation}
\label{eq:lunarreward}
    r_t =
    \begin{cases*}
      \delta _{state} - m_t*0.30 - lr_t*0.03 -100  & after crash \\
      \delta _{state} - m_t*0.30 - lr_t*0.03 +100 & after landing \\
      \delta _{state} - m_t*0.30 - lr_t*0.03  & otherwise 
    \end{cases*}
\end{equation}
