%!TEX root = ../authorinstr.tex

\section{Application}

In this section, the domains used to test the different algorithms will be described. GD-SARSA, CACLA and NFAC are compared in two continuous environments: MountainCar\cite{openaimountaincar} and LunarLander\cite{openailunarlander}. Both environments are OpenAI Gym environments\cite{openaigym}, which is a toolkit for comparing reinforcement learning algorithms.  Agent performance is measured by looking at the average reward over the best 100 epochs and the amount of epchos where the agent reached its goal, within the last 100 epochs. The reward function is given by the OpenAI environment and shown in more detail in their respective subsections. Each simulation run consisted of 2000 epochs, each of which had a maximum of 10000 time steps.



\subsection{MountainCar}
In the MountainCar environment a car is situated in between two mountains. Its goal is to reach the top of the rightmost mountain. In order to do this, it first has to drive up the left mountain in order to generate enough momentum to reach the top of the rightmost mountain. 

The action space of the MountainCar environment contains continouos numbers in the range [$-1$,$1$]. This dictates the force applied to the car in either the left or right direction. The state space consists of two continouos numbers:
\begin{itemize}
    \item[] The car position, in the range [$-1.2$,$0.6$]
    \item[] The car velocity, in the range [$-0.07$,$0.07$]
\end{itemize}

The reward function, where $r_t$ is the reward at time step $t$ and $v_t$ is the car velocity at time step $t$, is given by the OpenAI Gym environment and defined as follows:
\begin{equation}
    r_t =
    \begin{cases*}
      +100 -v_t^2 * 0.1 & if goal is reached \\
      -v_t^2 * 0.1 & otherwise
    \end{cases*}
\end{equation}

The total reward is defined as the sum of all rewards gained during an epoch.

\subsection{LunarLander}
In the LunarLander environment, the lunar lander's goal is to safely land on the lunar surface. It can do this by firing its left or right engine and controlling its overall thrust power. The action space of the LunarLander environment consists of vectors with two continouos numbers in the range [$-1$,$1$]: one for controlling the left or right engine and one for controlling the main engine. In the first case, a value in between -1.0 and -0.5 means the left engine is firing, a value in between 0.5 and 1.0 means the right engine is firing, and any other value means neither engine is firing. For the main engine, a value in between -1.0 and 0 means that the main engine is not firing, while from 0 to 1.0 the engine is throttled from 50\% to 100\% power. The LunarLander state space consists of 6 continouos numbers, x-position, y-position, x-velocity, y-velocity, angle and rotation respectively, in the range [$-1$,$1$] and two booleans variables, indicating ground contact for the left and right leg respectively.

For the reward function, as given by the OpenAI Gym environment, the state at time t $st_t$ is determined. It is defined as such:

\begin{equation} 
    st_t = -100 * d_t - 100 * v_t - 100 * a_t + 10*l_t + 10*r_t
\end{equation}

where $d_t$ is the distance from the lunar lander to landing zone at time $t$, $v_t$ is the velocity of the lunar lander at time $t$, $a_t$ is the lunar lander's angle at time $t$, and $l_t$ and $r_t$ are booleans of respectively the left and right foot touching the ground at time $t$.  

Using this state $st_t$, the change in state which is needed for the reward function can be determined (equation \ref{eq:statet}). 

\begin{equation}
\label{eq:statet}
    \delta _{state} =
    \begin{cases*}
      0  & for epoch 1 \\
      st_t - st _{t-1} & otherwise 
    \end{cases*}
\end{equation}

The reward function, where $r_t$ is the reward at time step $t$, $m_t$ is the main engine power and $lr_t$ is the left-right engine power, is given here (equation \ref{eq:lunarreward}):

\begin{equation}
\label{eq:lunarreward}
    r_t =
    \begin{cases*}
      \delta _{state} - m_t*0.30 - lr_t*0.03 -100  & after crash \\
      \delta _{state} - m_t*0.30 - lr_t*0.03 +100 & after landing \\
      \delta _{state} - m_t*0.30 - lr_t*0.03  & otherwise 
    \end{cases*}
\end{equation}

The total reward is defined as the sum of all rewards gained during an epoch.
