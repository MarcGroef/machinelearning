%!TEX root = ../authorinstr.tex

\section{Introduction}

Designing an optimal controller for a given environment and goal can be a complex task. Reinforcement Learning (RL) techniques exist to address these tasks by learning a policy that results in choosing optimal actions in a given state. This way agents are able to learn complex behaviour, such as playing backgammon \cite{tesauro2002programming} or chess \cite{baxter1999knightcap},
real life quadripedal walking \cite{kohl2004policy}, or autonomous simulated driving \cite{}. \\  %%ADD ALPHAGO


In initial stages the agent performs random actions while learning to approximate received reward after the chosen actions. This should enable the agent to learn an optimal policy for the defined reward function. For this learning process multiple techniques exist. In this paper, the performance of a recently published technique, Neural Fitted Actor Critic (NFAC), will be compared to two well-established RL techniques: SARSA and CACLA.   

Recently new environments have arisen for agents to engage in, such as OpenAI \cite{openaigym}. Here multiple environments are presented, such as playing Atari games and more classic problems such as MountainCar and CartPole. For this paper, the continuous variants of the LunarLander and MountainCar environments, where both the action space and state space are continuous, were chosen (shown in figure \ref{fig:mountainlunar}). Discrete state spaces are known to work well with RL techniques, although continuous state spaces often present more of a challenge \cite{cetina2008multilayer}. \\

%\subsection*{SARSA}
In \cite{nichols2015continuous}, Nichols et al. describe an approach of using SARSA together with a technique based on Newtons Method (NM) to obtain continuous action selection. For our benchmarking task, SARSA will be used in a similar way. Instead of using NM for continuous action selection, we will use Gradient Descent (GD) to obtain actions resulting in a higher expected Q-value.

%\subsection*{CALCA}
%%TODO shortly describe prev work CACLA
%\subsection*{NFAC}
%%TODO shorty describe prev work NFAC





%Furthermore the required level
%of discritization can not be known in all 11 cases, and having to discover it is time intensive \cite{van2007reinforcement}. %citatie naar de cacla paper
%This potential problem can be solved by keeping the state and action space continuous in the environment representation of the agent. \\
%Several new reinforcement learning algorithms and adaptations of existing reinforcement algorithms were developed to model continuous
%state and action spaces. In this paper we will benchamark one such algorithm, named Neural Fitted Actor Critic (NFAC), which was develeped in August 2016 \cite{zimmer2016neural}
%against two established (at the time of the writing of this paper) continuous reinforcement learning algorithms, named CACLA (Continuous Actor Critic Learning Automaton)
%\cite{van2007reinforcement}
%and SARSA (State Action Reward State Action) adapted for continuous state- and action-spaces \cite{nichols2014application} with gradient descent (referred to in this paper as GD-SARSA). \\
First a formal background will be given on connectionist Reinforcement Learning with the use of Multi-Layer Perceptrons (MLP) as function approximators, and on extending this to continuous state and action spaces. In the methods section the implementation of the three algorithms that are compared in this paper are explained in detail. After that, setup of the experiments will be discussed. Finally the results will be presented and evaluated, and the final conclusions will be drawn regarding NFAC performance compared to CACLA and GD-SARSA.

\begin{figure}[t]
 \centering 
    \includegraphics[width = 0.7\columnwidth]{figs/mountainlunar.png}
 \caption{Left: MountainCar Environment, Right: LunarLander Environment.}
\label{fig:mountainlunar}
\end{figure}
