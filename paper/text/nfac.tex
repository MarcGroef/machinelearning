%!TEX root = ../authorinstr.tex

\section{Neural Fitted Actor Critic}
Another RL-method that uses an AC system is Neural Fitted Actor-Critic (NFAC). Where CACLA is an offline AC algorithm, NFAC is an online AC algorithm. This means that a data collection $D_{\pi}$ is built which is filled with experience tuples of the form \{$s_{t}$, $u_{t}$, $a_{t}$, $r_{t+1}$, $s_{t+1}$\}. $s_{t}$ is the state at time $t$, $u_{t}$ is the action the Actor MLP provides, $a_{t}$ is the exploration action which can deviate from $u_{t}$, the reward at time $t+1$ is given by $r_{t+1}$, and finally $s_{t+1}$ is the resulting state after applying $u_{t}$. Every time the agent interacts in the environment, $D_{\pi}$ will be extended with one tuple.

In CACLA, first the Critic is updated and afterwards the Actor. However since the Critic is dependent on the value of the Actor when updating in batches, the Actor is updated first. The Actor is updated towards the exploration action if $\delta > 0$. This is identical for CACLA. NFAC deviates from CACLA by also updating the Actor when $\delta \leq 0$. In that case the update of the Actor is towards the action that the Actor MLP provides. The Critic update is similar to CACLA, but in NFAC the Critic is updated for each experience in $D_{\pi}$. Note that the data collection needs to be emptied after every epoch since the targets used in estimating the new value function are dependent on the current value function. 

