%!TEX root = ../authorinstr.tex

\begin{abstract}
<<<<<<< HEAD
\noindent Most intelligent behaviour is hard to model by using plain rules, but relatively easy to model using Reinforcement Learning. Three different Reinforcement Learning algorithms have been tested on two different OpenAI Gym environments (MountainCar and LunarLander) in which continuous states and actions are used. SARSA, Neural Fitted Actor-Critic and CACLA are compared and their performance is evaluated. The goal is to see whether the novel algorithm Neural Fitted Actor-Critic outperforms the two more established algorithms: CACLA and SARSA. Previous work \cite{zimmer2016neural} indicates that Neural Fitted Actor-Critic outperforms CACLA in terms of speed of converge and in quality of the learnt policy. The results indicate that ... and we found that ...
=======
Most intelligent behaviour is hard to model by using plain rules, but relatively easy to model using Reinforcement Learning. Three different Reinforcement Learning algorithms have been tested on two different OpenAI Gym environments (MountainCar and LunarLander) in which continuous state and action spaces are used. SARSA, Neural Fitted Actor-Critic and CACLA are compared and their performance is evaluated. The goal is to see whether the novel algorithm Neural Fitted Actor-Critic outperforms the two more established algorithms: CACLA and SARSA. Previous work \cite{zimmer2016neural} indicates that Neural Fitted Actor-Critic outperforms CACLA in terms of speed of converge and in quality of the learnt policy. The results indicate that ... and we found that ...
>>>>>>> b7369e291b6ed75aaca14e1a5240f1957664ab1f
\end{abstract}
