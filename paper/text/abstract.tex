%!TEX root = ../authorinstr.tex

\begin{abstract}
\noindent Most intelligent behaviour is hard to model by using plain rules, but relatively easy to model using Reinforcement Learning. Three different Reinforcement Learning algorithms have been tested on two different OpenAI Gym environments (MountainCar and LunarLander) in which continuous states and action spaces are used. SARSA, NFAC and CACLA are compared and their performance is evaluated. The goal is to see whether the novel algorithm NFAC outperforms the two more established algorithms: CACLA and SARSA. Previous work \cite{zimmer2016neural} indicates that Neural Fitted Actor-Critic outperforms CACLA in terms of speed of converge and in quality of the learnt policy. The results indicate that this is not the case, and that SARSA outperforms both CACLA and NFAC in the MountainCar and LunarLander environments.

\end{abstract}
